services:
  backend:
    build:
      context: .
      dockerfile: docker/main/Dockerfile
    container_name: backend
    env_file:
      - .env
    volumes:
      - ./app:/app
      - ./site-front4:/site-front4
      - ./certs:/app/certs:ro
    ports:
      - "8000:8000"
    depends_on:
      - postgres
      - kafka1
      - kafka2
      - redis
    networks:
      - web_app

  nginx:
    image: nginx:alpine
    container_name: nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf
      - ./site-front4:/var/www/static
    depends_on:
      - backend
    networks:
      - web_app

  postgres:
    image: postgres:latest
    container_name: postgres
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_DB=${DB_PROD_NAME}
      - POSTGRES_USER=${DB_PROD_USER}
      - POSTGRES_PASSWORD=${DB_PROD_PASS}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - web_app
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U your_user -d your_db_name" ]
      interval: 10s
      timeout: 5s
      retries: 5

  pgadmin:
    image: dpage/pgadmin4
    container_name: pgadmin
    environment:
      - PGADMIN_DEFAULT_EMAIL=${PGADMIN_EMAIL}
      - PGADMIN_DEFAULT_PASSWORD=${PGADMIN_PASSWORD}
    ports:
      - "5050:80"
    depends_on:
      - postgres
    networks:
      - web_app

  kafka1:
    image: bitnami/kafka:3.6.1
    container_name: kafka1
    ports:
      - "9092:9092"  # INTERNAL (SASL_PLAINTEXT)
      - "9093:9093"  # EXTERNAL (SASL_SSL)
      - "9101:9101"  # CONTROLLER (PLAINTEXT)
    environment:
      # KRaft режим - основные настройки
      KAFKA_ENABLE_KRAFT: "yes"
      KAFKA_CFG_PROCESS_ROLES: "controller,broker"
      KAFKA_CFG_NODE_ID: "1"
      KAFKA_CFG_CONTROLLER_QUORUM_VOTERS: "1@kafka1:9101,2@kafka2:9101"
      KAFKA_CFG_CONTROLLER_LISTENER_NAMES: "CONTROLLER"

      # Listeners с аутентификацией
      KAFKA_CFG_LISTENERS: "INTERNAL://0.0.0.0:9092,EXTERNAL://0.0.0.0:9093,CONTROLLER://0.0.0.0:9101"
      KAFKA_CFG_ADVERTISED_LISTENERS: "INTERNAL://kafka1:9092,EXTERNAL://localhost:9093"
      KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP: "INTERNAL:SASL_PLAINTEXT,EXTERNAL:SASL_SSL,CONTROLLER:PLAINTEXT"
      KAFKA_CFG_INTER_BROKER_LISTENER_NAME: "INTERNAL"

      # SASL настройки
      KAFKA_CFG_SASL_ENABLED_MECHANISMS: "PLAINTEXT"

      # SSL автогенерация
      KAFKA_TLS_TYPE: "PEM"
      KAFKA_TLS_CLIENT_AUTH: "none"
      KAFKA_CFG_SSL_KEYSTORE_TYPE: "PEM"
      KAFKA_CFG_SSL_TRUSTSTORE_TYPE: "PEM"
      KAFKA_CFG_SSL_KEYSTORE_LOCATION: "/opt/bitnami/kafka/config/certs/server-cert.pem"
      KAFKA_CFG_SSL_KEYSTORE_KEY_LOCATION: "/opt/bitnami/kafka/config/certs/server-key.pem"
      KAFKA_CFG_SSL_TRUSTSTORE_LOCATION: "/opt/bitnami/kafka/config/certs/ca-cert.pem"

      # Cluster ID
      KAFKA_KRAFT_CLUSTER_ID: "abcdefghijklmnopqrstuv"

      # Основные настройки
      KAFKA_CFG_NUM_PARTITIONS: "6"
      KAFKA_CFG_DEFAULT_REPLICATION_FACTOR: "2"
      KAFKA_CFG_MIN_INSYNC_REPLICAS: "1"
      KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_HEAP_OPTS: "-Xmx1G -Xms1G"

      # Разрешаем plaintext для controller
      ALLOW_PLAINTEXT_LISTENER: "yes"
    volumes:
      - kafka1_data:/bitnami/kafka
      - ./certs:/opt/bitnami/kafka/config/certs:ro
    networks:
      - web_app

  kafka2:
    image: bitnami/kafka:3.6.1
    container_name: kafka2
    ports:
      - "9094:9094"  # INTERNAL (SASL_PLAINTEXT)
      - "9095:9095"  # EXTERNAL (SASL_SSL)
      - "9102:9102"  # CONTROLLER (PLAINTEXT)
    environment:
      # KRaft режим - основные настройки
      KAFKA_ENABLE_KRAFT: "yes"
      KAFKA_CFG_PROCESS_ROLES: "controller,broker"
      KAFKA_CFG_NODE_ID: "2"
      KAFKA_CFG_CONTROLLER_QUORUM_VOTERS: "1@kafka1:9101,2@kafka2:9101"
      KAFKA_CFG_CONTROLLER_LISTENER_NAMES: "CONTROLLER"

      # Listeners с аутентификацией
      KAFKA_CFG_LISTENERS: "INTERNAL://0.0.0.0:9094,EXTERNAL://0.0.0.0:9095,CONTROLLER://0.0.0.0:9102"
      KAFKA_CFG_ADVERTISED_LISTENERS: "INTERNAL://kafka2:9094,EXTERNAL://localhost:9095"
      KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP: "INTERNAL:SASL_PLAINTEXT,EXTERNAL:SASL_SSL,CONTROLLER:PLAINTEXT"
      KAFKA_CFG_INTER_BROKER_LISTENER_NAME: "INTERNAL"

      # SASL настройки
      KAFKA_CFG_SASL_ENABLED_MECHANISMS: "PLAINTEXT"

      # SSL автогенерация
      KAFKA_TLS_TYPE: "PEM"
      KAFKA_TLS_CLIENT_AUTH: "none"
      KAFKA_CFG_SSL_KEYSTORE_TYPE: "PEM"
      KAFKA_CFG_SSL_TRUSTSTORE_TYPE: "PEM"
      KAFKA_CFG_SSL_KEYSTORE_LOCATION: "/opt/bitnami/kafka/config/certs/server-cert.pem"
      KAFKA_CFG_SSL_KEYSTORE_KEY_LOCATION: "/opt/bitnami/kafka/config/certs/server-key.pem"
      KAFKA_CFG_SSL_TRUSTSTORE_LOCATION: "/opt/bitnami/kafka/config/certs/ca-cert.pem"

      # Cluster ID
      KAFKA_KRAFT_CLUSTER_ID: "abcdefghijklmnopqrstuv"

      # Основные настройки
      KAFKA_CFG_NUM_PARTITIONS: "6"
      KAFKA_CFG_DEFAULT_REPLICATION_FACTOR: "2"
      KAFKA_CFG_MIN_INSYNC_REPLICAS: "1"
      KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_HEAP_OPTS: "-Xmx1G -Xms1G"

      # Разрешаем plaintext для controller
      ALLOW_PLAINTEXT_LISTENER: "yes"
    volumes:
      - kafka2_data:/bitnami/kafka
      - ./certs:/opt/bitnami/kafka/config/certs:ro
    networks:
      - web_app

  redis:
    image: 'redis:latest'
    container_name: redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
  
  prometheus:
    image: 'docker.io/bitnami/prometheus:latest'
    container_name: prometheus
    ports: 
      - "9090:9090"
    volumes:
      - ./prometheus/prometheus.yml:/opt/bitnami/prometheus/conf/prometheus.yml:ro
      - prometheus_data:/opt/bitnami/prometheus/data
    command:
      - '--config.file=/opt/bitnami/prometheus/conf/prometheus.yml'
      - '--storage.tsdb.path=/opt/bitnami/prometheus/data'
      - '--web.console.libraries=/opt/bitnami/prometheus/conf/console_libraries'
      - '--web.console.templates=/opt/bitnami/prometheus/conf/consoles'
    restart: unless-stopped
    networks:
      - web_app
  
  grafana:
    image: 'docker.io/bitnami/grafana:latest'
    container_name: grafana
    ports:
      - "3000:3000"
    depends_on:
      - prometheus
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=12345
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_INSTALL_PLUGINS=grafana-clock-panel,grafana-simple-json-datasource
    volumes:
      - grafana_data:/opt/bitnami/grafana/data
      - ./grafana/provisioning:/opt/bitnami/grafana/conf/provisioning
    restart: unless-stopped

  elasticsearch:
    image: elasticsearch:7.8.0
    environment:
      - discovery.type=single-node
      - ES_JAVA_OPTS=-Xms512m -Xmx512m
      - xpack.security.enabled=false
    ports:
      - "9200:9200"
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    networks:
      - web_app
  
  logstash:
    image: logstash:7.8.0
    volumes:
      - ./logstash/pipeline:/usr/share/logstash/pipeline
    ports:
      - "5044:5044"
    depends_on:
      - elasticsearch
    networks:
      - web_app
    
  kibana:
    image: kibana:7.8.0
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    depends_on:
      - elasticsearch
    networks:
      - web_app

  consumer1:
    build:
      context: .
      dockerfile: docker/consumer/Dockerfile
    command: python consumer.py
    env_file:
      - .env
    volumes:
      - ./certs:/app/certs:ro
    environment:
      CONSUMER_ID: "1"
  consumer2:
    build:
      context: .
      dockerfile: docker/consumer/Dockerfile
    command: python consumer.py
    env_file:
      - .env
    volumes:
      - ./certs:/app/certs:ro
    environment:
      CONSUMER_ID: "2"
  consumer3:
    build:
      context: .
      dockerfile: docker/consumer/Dockerfile
    command: python consumer.py
    env_file:
      - .env
    volumes:
      - ./certs:/app/certs:ro
    environment:
      CONSUMER_ID: "3"
  consumer4:
    build:
      context: .
      dockerfile: docker/consumer/Dockerfile
    command: python consumer.py
    env_file:
      - .env
    volumes:
      - ./certs:/app/certs:ro
    environment:
      CONSUMER_ID: "4"
  consumer5:
    build:
      context: .
      dockerfile: docker/consumer/Dockerfile
    command: python consumer.py
    env_file:
      - .env
    volumes:
      - ./certs:/app/certs:ro
    environment:
      CONSUMER_ID: "5"
  consumer6:
    build:
      context: .
      dockerfile: docker/consumer/Dockerfile
    command: python consumer.py
    env_file:
      - .env
    volumes:
      - ./certs:/app/certs:ro
    environment:
      CONSUMER_ID: "6"

  # Celery воркер для email кампаний
  celery-worker-email:
    build:
      context: .
      dockerfile: docker/main/Dockerfile
    container_name: celery-worker-email
    command: celery -A utils.celery_conf:celery_app worker --queues=email_campaigns -c 4 -l INFO --hostname=email-worker@%h
    env_file:
      - .env
    volumes:
      - ./app:/app
    depends_on:
      - redis
      - postgres  # если воркеры работают с БД
    networks:
      - web_app
    restart: unless-stopped

  # Celery воркер для подписок
  celery-worker-subs:
    build:
      context: .
      dockerfile: docker/main/Dockerfile
    container_name: celery-worker-subs
    command: celery -A utils.celery_conf:celery_app worker --queues=subscriptions_updates -c 2 -l INFO --hostname=subs-worker@%h
    env_file:
      - .env
    volumes:
      - ./app:/app
    depends_on:
      - redis
      - postgres
    networks:
      - web_app
    restart: unless-stopped

  # Celery Beat (планировщик)
  celery-beat:
    build:
      context: .
      dockerfile: docker/main/Dockerfile
    container_name: celery-beat
    command: celery -A utils.celery_conf:celery_app beat -l INFO --scheduler django_celery_beat.schedulers:DatabaseScheduler
    env_file:
      - .env
    volumes:
      - ./app:/app
    depends_on:
      - redis
      - postgres
      - backend  # запускаем после основного приложения
    networks:
      - web_app
    restart: unless-stopped

  # Flower (опционально - веб-интерфейс для мониторинга)

networks:
  web_app:
    driver: bridge

volumes:
  redis_data:
  prometheus_data:
  grafana_data:
  elasticsearch_data:
  kafka1_data:
  kafka2_data:
  postgres_data: